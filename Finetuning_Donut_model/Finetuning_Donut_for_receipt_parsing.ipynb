{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Donut for receipt parsing\n",
        "With this code, you can fine-tune the powerful [Donut](https://huggingface.co/docs/transformers/model_doc/donut) to your own dataset, allowing you to create a custom model that can accurately extract the information you need from receipts. By tailoring the model to your specific requirements, you can ensure that it provides accurate and relevant output, improving efficiency and streamlining your workflow. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v-TXj9qKjgLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Getting Started\n",
        "\n",
        "### Downloading Donut and installing packages\n",
        "To begin, we will clone the [Donut](https://huggingface.co/docs/transformers/model_doc/donut) code repository from GitHub. Once downloaded, we will navigate to the directory containing the repository and proceed to install all the necessary packages and dependencies required for the Donut model (`!cd donut && pip install .`)."
      ],
      "metadata": {
        "id": "TdC3z0bbl-yP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tBsgDypDNyT"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/clovaai/donut.git\n",
        "!cd donut && pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Dataset\n"
      ],
      "metadata": {
        "id": "JCBxd7mRmLZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload\n",
        "To use your data for finetuning [Donut](https://huggingface.co/docs/transformers/model_doc/donut), follow these steps:\n",
        "\n",
        "1. Prepare your dataset as described in the repository [https://github.com/Inesence/REpro/tree/main/Creating_dataset](https://github.com/Inesence/REpro/tree/main/Creating_dataset).\n",
        "2. Upload your dataset to your Github repository.\n",
        "3. Clone and the data from Github by using the following code(replace <Inesence/receipts_LV> with the name of your repository):"
      ],
      "metadata": {
        "id": "jmWLdI6nsFhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvxf0sJkKHeD"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# clone repository\n",
        "git clone https://github.com/Inesence/receipts_LV_public.git\n",
        "# copy data\n",
        "cp -r receipts_LV_public/data ./"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsetting\n",
        "\n",
        "If you have a larger dataset that exceeds the memory capacity of Google Colab or your local resources when traing the model, you may need to cut your data into a smaller subset that can be easily handled. For me the cut-off was 200-220 image-key pairs. \n",
        "\n",
        "If you have enough resources to handle your data, skip this step."
      ],
      "metadata": {
        "id": "pLOfTqlRoSBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mozZnxQqWqrM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "threshold = 200 # Enter your cut-off value here\n",
        "main_folder = \"/content/receipts_LV_public\" # Enter your main folder path here\n",
        "\n",
        "# define the path to the folder containing the files to delete\n",
        "folder_path = main_folder + \"/data/img/\"\n",
        "\n",
        "# loop through the files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "  \n",
        "  # check if the file name contains \"img\" and a number, and if it has a file extension\n",
        "  if \"jpg\" in file_name and file_name[:-4].isdigit():\n",
        "    \n",
        "    # extract the number from the file name\n",
        "    file_num = int(file_name[:-4])\n",
        "    \n",
        "    # check if the file number is less than or equal to 40\n",
        "    if file_num <= threshold:\n",
        "      \n",
        "      # build the full path to the file\n",
        "      file_path = folder_path + file_name\n",
        "      \n",
        "      # delete the file\n",
        "      os.remove(file_path)\n",
        "      \n",
        "# define the path to the folder containing the files to delete\n",
        "folder_path = main_folder + \"/data/key/\"\n",
        "\n",
        "# loop through the files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "  \n",
        "  # check if the file name contains \"img\" and a number, and if it has a file extension\n",
        "  if \"json\" in file_name and file_name[:-5].isdigit():\n",
        "    \n",
        "    # extract the number from the file name\n",
        "    file_num = int(file_name[:-5])\n",
        "    \n",
        "    # check if the file number is less than or equal to 40\n",
        "    if file_num <= threshold:\n",
        "      \n",
        "      # build the full path to the file\n",
        "      file_path = folder_path + file_name\n",
        "      \n",
        "      # delete the file\n",
        "      os.remove(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing your Dataset\n",
        "\n",
        "**The following code and explanations have been directly sourced from [Philipp Schmid's guide](https://www.philschmid.de/fine-tuning-donut), and full credit goes to him.**\n",
        "\n",
        "The next step is to prepare the dataset so that it conforms to the format that the Donut model requires. It is crucial to create a `metadata.json` file that stores important information about the images, such as the desired output.\n"
      ],
      "metadata": {
        "id": "qw7qAYBMrgZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i4tKKSv1aYhM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# define paths\n",
        "base_path = Path(\"data\")\n",
        "metadata_path = base_path.joinpath(\"key\")\n",
        "image_path = base_path.joinpath(\"img\")\n",
        "# define metadata list\n",
        "metadata_list = []\n",
        "\n",
        "# parse metadata\n",
        "for file_name in metadata_path.glob(\"*.json\"):\n",
        "  with open(file_name, \"r\") as json_file:\n",
        "    # load json file\n",
        "    data = json.load(json_file)\n",
        "    # create \"text\" column with json string\n",
        "    text = json.dumps(data)\n",
        "    # add to metadata list if image exists\n",
        "    if image_path.joinpath(f\"{file_name.stem}.jpg\").is_file():\n",
        "      metadata_list.append({\"text\":text,\"file_name\":f\"{file_name.stem}.jpg\"})\n",
        "      # delete json file\n",
        "\n",
        "# write jsonline file\n",
        "with open(image_path.joinpath('metadata.jsonl'), 'w') as outfile:\n",
        "    for entry in metadata_list:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "\n",
        "# remove old meta data\n",
        "shutil.rmtree(metadata_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the data can be loaded using the `imagefolder` feature of `datasets` package."
      ],
      "metadata": {
        "id": "9e2QQAaquQBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOEVzJTra-O9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from datasets import load_dataset\n",
        "\n",
        "# define paths\n",
        "base_path = Path(\"data\")\n",
        "metadata_path = base_path.joinpath(\"key\")\n",
        "image_path = base_path.joinpath(\"img\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"imagefolder\", data_dir=image_path, split=\"train\")\n",
        "\n",
        "print(f\"Dataset has {len(dataset)} images\")\n",
        "print(f\"Dataset features are: {dataset.features.keys()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Donut model is a type of sequence-to-sequence model that has a vision encoder and a text decoder. During the fine-tuning process, we want the model to generate text based on the images that we provide as input. To do so, we need to tokenize and preprocess the text before using it as input to the model.\n",
        "\n",
        "In order to tokenize the text, we first need to transform the JSON string into a format that is compatible with the Donut model. To make this process easier, the ClovaAI team has developed a method called `json2token`, which we can use to create Donut-compatible documents from the JSON data."
      ],
      "metadata": {
        "id": "QpgJO2opveCO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "6c63bd60cc544de2b265ebc6a4139788",
            "f2ef285f612b4908be8e74aa4df0c464",
            "77339d30b228482db75af96ee72250c5",
            "4309375ac43748598f4fbe2351e6e01a",
            "c487f52706064bd28558710d414bd38b",
            "d09fa8ade27943038409e06fc5d1bb44",
            "90883dc7f51c4fcabefd30b0d8fda79e",
            "ab7e51238200448480a7e271d322646d",
            "80644078719c40a790a9366e73448d1c",
            "f1e4cba6db24472fa8cbdca225e0752f",
            "da11db9017e34214bd1ae943186c3126"
          ]
        },
        "id": "-5Tva-BXbhVf",
        "outputId": "3cfe576c-7603-42db-c5fb-ef0da3a9d053"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/221 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c63bd60cc544de2b265ebc6a4139788"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "new_special_tokens = [] # new tokens which will be added to the tokenizer\n",
        "task_start_token = \"<s>\"  # start of task token\n",
        "eos_token = \"</s>\" # eos token of tokenizer\n",
        "\n",
        "def json2token(obj, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
        "    \"\"\"\n",
        "    Convert an ordered JSON object into a token sequence\n",
        "    \"\"\"\n",
        "    if type(obj) == dict:\n",
        "        if len(obj) == 1 and \"text_sequence\" in obj:\n",
        "            return obj[\"text_sequence\"]\n",
        "        else:\n",
        "            output = \"\"\n",
        "            if sort_json_key:\n",
        "                keys = sorted(obj.keys(), reverse=True)\n",
        "            else:\n",
        "                keys = obj.keys()\n",
        "            for k in keys:\n",
        "                if update_special_tokens_for_json_key:\n",
        "                    new_special_tokens.append(fr\"<s_{k}>\") if fr\"<s_{k}>\" not in new_special_tokens else None\n",
        "                    new_special_tokens.append(fr\"</s_{k}>\") if fr\"</s_{k}>\" not in new_special_tokens else None\n",
        "                output += (\n",
        "                    fr\"<s_{k}>\"\n",
        "                    + json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
        "                    + fr\"</s_{k}>\"\n",
        "                )\n",
        "            return output\n",
        "    elif type(obj) == list:\n",
        "        return r\"<sep/>\".join(\n",
        "            [json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
        "        )\n",
        "    else:\n",
        "        # excluded special tokens for now\n",
        "        obj = str(obj)\n",
        "        if f\"<{obj}/>\" in new_special_tokens:\n",
        "            obj = f\"<{obj}/>\"  # for categorical special tokens\n",
        "        return obj\n",
        "\n",
        "\n",
        "def preprocess_documents_for_donut(sample):\n",
        "    # create Donut-style input\n",
        "    text = json.loads(sample[\"text\"])\n",
        "    d_doc = task_start_token + json2token(text) + eos_token\n",
        "    # convert all images to RGB\n",
        "    image = sample[\"image\"].convert('RGB')\n",
        "    return {\"image\": image, \"text\": d_doc}\n",
        "\n",
        "proc_dataset = dataset.map(preprocess_documents_for_donut)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step involves two important tasks: tokenizing the text and encoding the images as tensors. To achieve this, we need to use the `DonutProcessor`. Additionally, we need to incorporate new special tokens into the tokenizer and adjust the image size during processing to reduce memory usage and accelerate the training process.\n",
        "\n",
        "*Since my receipts are in Latvian language in the following code, I added the missing Latvian characters to the tokenizer by using `latvian_chars = ['ā', 'č', 'ē', 'ī', 'ķ', 'ļ', 'ņ', 'š', 'ū', 'ž']` and then applying `processor.tokenizer.add_tokens(latvian_chars)` This was necessary because not all of the Latvian characters were present in the tokenizer, and the addition of these characters to the tokenizer ensures that they will be properly recognized and processed during training.* \n",
        "\n",
        "*If you are training the Donut model in your language, it is important to ensure that the DonutProcessor module has all the necessary tokens to accurately generate the desired text output. In the event that some tokens are missing, you may need to add them manually to the tokenizer.*\n",
        "\n",
        "*However, if the tokenizer does not support your language at all, you may need to consider using a different decoder altogether. It's important to note that the quality of the text output will be heavily dependent on the quality of the tokenizer and the availability of appropriate tokens. More about it in discussion here: [Finetune Donut with new tokenizer](https://discuss.huggingface.co/t/finetune-donut-with-new-tokenizer/30567/).*"
      ],
      "metadata": {
        "id": "p6yrkDYgxBYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To test if the tokenizer has characters in your language you can use the code below. Just input the text in your language with characters different from english and test it. For example the tokenizer had all but three latvian characters - ļ,ņ and ķ.*"
      ],
      "metadata": {
        "id": "drbt0OHhWuLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DonutProcessor\n",
        "\n",
        "# Load processor\n",
        "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "\n",
        "# Input text\n",
        "text = \"{'āēīõūļķčšņüäößü'}\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokens = processor.tokenizer(text, add_special_tokens=True)\n",
        "\n",
        "# Decode the tokens back to the original form\n",
        "decoded_text = processor.tokenizer.decode(tokens['input_ids'])\n",
        "\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"Decoded text: {decoded_text}\")"
      ],
      "metadata": {
        "id": "T9QBFSnvVWLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proceed here if your language is supported by the tokenizer. "
      ],
      "metadata": {
        "id": "S85haqeUXVO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RrcIvAJSAVB"
      },
      "outputs": [],
      "source": [
        "from transformers import DonutProcessor\n",
        "\n",
        "# Load processor\n",
        "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "\n",
        "# add missing Latvian tokens to tokenizer\n",
        "latvian_chars = ['ā', 'č', 'ē', 'ī', 'ķ', 'ļ', 'ņ', 'š', 'ū', 'ž']\n",
        "processor.tokenizer.add_tokens(latvian_chars)\n",
        "\n",
        "# add new special tokens to tokenizer\n",
        "processor.tokenizer.add_special_tokens({\"additional_special_tokens\": new_special_tokens + [task_start_token] + [eos_token]})\n",
        "\n",
        "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
        "# resizing the image to smaller sizes from [1920, 2560] to [960,1280]\n",
        "processor.feature_extractor.size = [720,960] # should be (width, height)\n",
        "processor.feature_extractor.do_align_long_axis = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below preprocesses data for fine-tuning the Donut model. It first converts the image into a tensor using the DonutProcessor, while also tokenizing the text in the sample. The tokenizer converts the text into a series of numerical IDs that the model can understand. The function then returns a dictionary with the image tensor, the tokenized text IDs (with padding), and the original text as a target sequence. Finally, the function is applied to the dataset using the map function, which applies the transformation to every sample in the dataset."
      ],
      "metadata": {
        "id": "JTsETABnz8Ll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "89b93309df1a453986c9b2829527b43b",
            "3317fa332f0848e7b3d66c15f4547629",
            "617f21037ab940ee90dfa71b307a3cd0",
            "ce6dd406c98349289a2a0cb8be6397f8",
            "c932d2cdb97f49b7900d06c73b334649",
            "bf4f9221cf0f4b86b7a638708340b2ce",
            "774d412c98bc45d585b735d609f582f4",
            "0d75980863e9404ebb19699aa2b8c8bc",
            "9761b9df138d45aa949de6ef6c2a1693",
            "55f31f6417ff4854912ef90db484151d",
            "baae9bea23e1476d9d160b482f9342db"
          ]
        },
        "id": "FCYIZxulUHCL",
        "outputId": "0dd310b1-6356-426d-a10e-1660ede5aa8d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/221 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89b93309df1a453986c9b2829527b43b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def transform_and_tokenize(sample, processor=processor, split=\"train\", max_length=512, ignore_id=-100):\n",
        "    # create tensor from image\n",
        "    #sample[\"text\"] = sample[\"text\"].encode('utf-8').decode('utf-8')\n",
        "    try:\n",
        "        pixel_values = processor(\n",
        "            sample[\"image\"], random_padding=split == \"train\", return_tensors=\"pt\"\n",
        "        ).pixel_values.squeeze()\n",
        "    except Exception as e:\n",
        "        print(sample)\n",
        "        print(f\"Error: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # tokenize document\n",
        "    input_ids = processor.tokenizer(\n",
        "        sample[\"text\"],\n",
        "        add_special_tokens=False,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )[\"input_ids\"].squeeze(0)\n",
        "\n",
        "    labels = input_ids.clone()\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = ignore_id  # model doesn't need to predict pad token\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"target_sequence\": sample[\"text\"]}\n",
        "\n",
        "# need at least 32-64GB of RAM to run this\n",
        "processed_dataset = proc_dataset.map(transform_and_tokenize,remove_columns=[\"image\",\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the dataset needs to be split into train and validation sets."
      ],
      "metadata": {
        "id": "CBQmxFtgzhs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQXUzvSjZD9a"
      },
      "outputs": [],
      "source": [
        "processed_dataset = processed_dataset.train_test_split(test_size=0.1)\n",
        "print(processed_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Finetuning Donut model\n",
        "\n",
        "**The following code and explanations have been directly sourced from [Philipp Schmid's guide](https://www.philschmid.de/fine-tuning-donut), and full credit goes to him.**"
      ],
      "metadata": {
        "id": "16anSxKW1VjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Hub\n",
        "To access your finetuned Donut model in the future, you will need to save it somewhere. One option is to use the Hugging Face Hub, a remote model versioning service. To do this, you will first need to [create an account with Hugging Face](https://huggingface.co/join). Once you have an account, you can log in to it from your notebook using the `notebook_login` utility provided by the `huggingface_hub` package. This will enable you to push your model to the Hub for versioning and sharing."
      ],
      "metadata": {
        "id": "0rSbZFEq1477"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GexJnlUWZ1FG"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "The next step is to initiate the training of our model. This is done by loading the pre-trained `naver-clova-ix/donut-base` model using the `VisionEncoderDecoderModel` class. The donut-base model, which is equipped with pre-trained weights, was originally introduced in the research paper \"OCR-free Document Understanding Transformer\" by Geewok et al..\n",
        "\n",
        "Furthermore, apart from loading our model, we also need to make some adjustments before we start training. These include resizing the embedding layer to align with any new tokens that may have been added, adjusting the image size of our encoder to fit our dataset, and incorporating tokens that will be required for future inference."
      ],
      "metadata": {
        "id": "QTj4PI3H3pBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw1mKSQaZLBG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig\n",
        "\n",
        "# Load model from huggingface.com\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "\n",
        "# Resize embedding layer to match vocabulary size\n",
        "new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
        "print(f\"New embedding size: {new_emb}\")\n",
        "# Adjust our image size and output sequence lengths\n",
        "model.config.encoder.image_size = processor.feature_extractor.size[::-1] # (height, width)\n",
        "model.config.decoder.max_length = len(max(processed_dataset[\"train\"][\"labels\"], key=len))\n",
        "\n",
        "# Add task token for decoder to start\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]\n",
        "\n",
        "# is done by Trainer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next thing we need is to train our sequence-to-sequence model. We'll be using a Seq2SeqTrainer from the transformers library for this. We need to set up some training parameters, like how many epochs we want to train for, and then create a trainer object using our model and training data. Finally, we can save our trained model to the Hugging Face model hub. **Make sure to name your model in this variable `hf_repository_id`.**"
      ],
      "metadata": {
        "id": "x07Ad4bGJLeI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QypwTvw3ZrYE"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# hyperparameters used for multiple args\n",
        "hf_repository_id = \"donut-base-finetuned-Latvian-receipts-v3\" # Enter your desired model name here\n",
        "\n",
        "# Arguments for training\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=hf_repository_id,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    predict_with_generate=True,\n",
        "    # push to hub parameters\n",
        "    report_to=\"tensorboard\",\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_model_id=hf_repository_id,\n",
        "    hub_token=HfFolder.get_token(),\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to train our model!"
      ],
      "metadata": {
        "id": "p2c7ggBzJ7U8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0MlesRZZwsi"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training process has been completed, push our processor onto the Hugging Face Hub."
      ],
      "metadata": {
        "id": "Pz0Js_SuKg4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXUevRopbgnr"
      },
      "outputs": [],
      "source": [
        "# Save processor and create model card\n",
        "processor.save_pretrained(hf_repository_id)\n",
        "trainer.create_model_card()\n",
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the fine-tuned model\n",
        "Now we will use our previously trained model to make predictions on a test document image. The run_prediction function is then defined to run inference on the image, by passing it through the model and generating a predicted output. The function takes in the test image, the model, and the processor as inputs. The image is processed and prepared for input into the model. Once the inference is complete, the predicted output is decoded and processed into a readable format. Finally, the predicted output and the truth sequence is printed for all test images."
      ],
      "metadata": {
        "id": "DYpIvhz4KsHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaOTb5vPbmwW"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import transformers\n",
        "from PIL import Image\n",
        "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# hidde logs\n",
        "transformers.logging.disable_default_handler()\n",
        "\n",
        "\n",
        "# Load our model from Hugging Face\n",
        "processor = DonutProcessor.from_pretrained(\"inesence/donut-base-finetuned-Latvian-receipts-v2\")\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"inesence/donut-base-finetuned-Latvian-receipts-v2\")\n",
        "\n",
        "# Move model to GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "def run_prediction(sample, model=model, processor=processor):\n",
        "    # prepare inputs\n",
        "    pixel_values = torch.tensor(test_sample[\"pixel_values\"]).unsqueeze(0)\n",
        "    task_prompt = \"<s>\"\n",
        "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # run inference\n",
        "    outputs = model.generate(\n",
        "        pixel_values.to(device),\n",
        "        decoder_input_ids=decoder_input_ids.to(device),\n",
        "        max_length=model.decoder.config.max_position_embeddings,\n",
        "        early_stopping=True,\n",
        "        pad_token_id=processor.tokenizer.pad_token_id,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id,\n",
        "        use_cache=True,\n",
        "        num_beams=1,\n",
        "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
        "        return_dict_in_generate=True,\n",
        "    )\n",
        "\n",
        "    # process output\n",
        "    prediction = processor.batch_decode(outputs.sequences)[0]\n",
        "    prediction = processor.token2json(prediction)\n",
        "\n",
        "    # load reference target\n",
        "    target = processor.token2json(test_sample[\"target_sequence\"])\n",
        "    return prediction, target\n",
        "\n",
        "# Load data from the test set\n",
        "for i in range(0,len(processed_dataset[\"test\"])):\n",
        "  test_sample = processed_dataset[\"test\"][i]\n",
        "  prediction, target = run_prediction(test_sample)\n",
        "  print(f\"Reference:\\n {target}\")\n",
        "  print(f\"Prediction:\\n {prediction}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to calculate the accuracy of the model by testing it on the processed dataset's test samples. The accuracy percentage is calculated by dividing the number of true matches by the total number of comparisons and multiplying it by 100. The result is printed on the console.\n",
        "\n",
        "*Philipp Schmid reported an accuracy of 75%, but in my case, the accuracy is 33.3%. This might be due to a lower number of training examples or different output needs. For example, I require the receipt number to be outputted, which can be difficult even for humans because of inconsistent placement, identification, length, and structure of receipt numbers.*"
      ],
      "metadata": {
        "id": "Qqz2fzOCNds9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD-p1I2zc8Tf"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# define counter for samples\n",
        "true_counter = 0\n",
        "total_counter = 0\n",
        "\n",
        "# iterate over dataset\n",
        "for sample in tqdm(processed_dataset[\"test\"]):\n",
        "  prediction, target = run_prediction(test_sample)\n",
        "  for s in zip(prediction.values(), target.values()):\n",
        "    if s[0] == s[1]:\n",
        "      true_counter += 1\n",
        "    total_counter += 1\n",
        "\n",
        "print(f\"Accuracy: {(true_counter/total_counter)*100}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further resources"
      ],
      "metadata": {
        "id": "BjegcM07PRsK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL_R2O1ad_Aj"
      },
      "source": [
        "**Philipp Schmid's step-by-step guide to fine-tune Donut:** https://www.philschmid.de/fine-tuning-donut\n",
        "\n",
        "**Neha Desaraju's step-by-step guide to fine-tune Donut:** https://towardsdatascience.com/ocr-free-document-understanding-with-donut-1acfbdf099be\n",
        "\n",
        "**Sample receipt-key dataset:** https://github.com/zzzDavid/ICDAR-2019-SROIE ; https://github.com/zzzDavid/ICDAR-2019-SROIE/tree/master/data/key\n",
        "\n",
        "**Niels Rogge tutorial on fine-tuning Donut**: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Donut/CORD/Fine_tune_Donut_on_a_custom_dataset_(CORD)_with_PyTorch_Lightning.ipynb\n",
        "\n",
        "\n",
        "**Donut on Hugging Face:** https://huggingface.co/docs/transformers/model_doc/donut\n",
        "\n",
        "**Training Tesseract-OCR with custom data:** https://saiashish90.medium.com/training-tesseract-ocr-with-custom-data-d3f4881575c0training-tesseract-ocr-with-custom-data-d3f4881575c0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c63bd60cc544de2b265ebc6a4139788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2ef285f612b4908be8e74aa4df0c464",
              "IPY_MODEL_77339d30b228482db75af96ee72250c5",
              "IPY_MODEL_4309375ac43748598f4fbe2351e6e01a"
            ],
            "layout": "IPY_MODEL_c487f52706064bd28558710d414bd38b"
          }
        },
        "f2ef285f612b4908be8e74aa4df0c464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d09fa8ade27943038409e06fc5d1bb44",
            "placeholder": "​",
            "style": "IPY_MODEL_90883dc7f51c4fcabefd30b0d8fda79e",
            "value": "Map: 100%"
          }
        },
        "77339d30b228482db75af96ee72250c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab7e51238200448480a7e271d322646d",
            "max": 221,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80644078719c40a790a9366e73448d1c",
            "value": 221
          }
        },
        "4309375ac43748598f4fbe2351e6e01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e4cba6db24472fa8cbdca225e0752f",
            "placeholder": "​",
            "style": "IPY_MODEL_da11db9017e34214bd1ae943186c3126",
            "value": " 221/221 [00:33&lt;00:00,  3.90 examples/s]"
          }
        },
        "c487f52706064bd28558710d414bd38b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d09fa8ade27943038409e06fc5d1bb44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90883dc7f51c4fcabefd30b0d8fda79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab7e51238200448480a7e271d322646d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80644078719c40a790a9366e73448d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1e4cba6db24472fa8cbdca225e0752f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da11db9017e34214bd1ae943186c3126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89b93309df1a453986c9b2829527b43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3317fa332f0848e7b3d66c15f4547629",
              "IPY_MODEL_617f21037ab940ee90dfa71b307a3cd0",
              "IPY_MODEL_ce6dd406c98349289a2a0cb8be6397f8"
            ],
            "layout": "IPY_MODEL_c932d2cdb97f49b7900d06c73b334649"
          }
        },
        "3317fa332f0848e7b3d66c15f4547629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf4f9221cf0f4b86b7a638708340b2ce",
            "placeholder": "​",
            "style": "IPY_MODEL_774d412c98bc45d585b735d609f582f4",
            "value": "Map: 100%"
          }
        },
        "617f21037ab940ee90dfa71b307a3cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d75980863e9404ebb19699aa2b8c8bc",
            "max": 221,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9761b9df138d45aa949de6ef6c2a1693",
            "value": 221
          }
        },
        "ce6dd406c98349289a2a0cb8be6397f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55f31f6417ff4854912ef90db484151d",
            "placeholder": "​",
            "style": "IPY_MODEL_baae9bea23e1476d9d160b482f9342db",
            "value": " 221/221 [01:29&lt;00:00,  2.02 examples/s]"
          }
        },
        "c932d2cdb97f49b7900d06c73b334649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "bf4f9221cf0f4b86b7a638708340b2ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774d412c98bc45d585b735d609f582f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d75980863e9404ebb19699aa2b8c8bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9761b9df138d45aa949de6ef6c2a1693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55f31f6417ff4854912ef90db484151d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baae9bea23e1476d9d160b482f9342db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}